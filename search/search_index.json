{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Software Availability On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time. Containers As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Home"},{"location":"#data-science-software-availability","text":"On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time.","title":"Data Science Software Availability"},{"location":"#containers","text":"As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Containers"},{"location":"GPU%20Monitoring/","text":"GPU Monitoring Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example. GPU Selection In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-monitoring","text":"Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example.","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-selection","text":"In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Selection"},{"location":"Singularity%20Containers/","text":"Nvidia Containers Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Singularity Containers"},{"location":"Singularity%20Containers/#nvidia-containers","text":"Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Nvidia Containers"},{"location":"building_python_packages/","text":"To build python packages for Theta GPU, there are two options: build on top of a bare-metal build (currently not available, but coming soon) or build on top of (and within) a singularity container. Additionally, you can build a new container from nvidia's docker images. Building on top of a container At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages. Reaching the outside world for pip packages You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY = http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py Building custom packages Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful. HDF5 You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success! Horovod Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Building Python Packages"},{"location":"building_python_packages/#building-on-top-of-a-container","text":"At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages.","title":"Building on top of a container"},{"location":"building_python_packages/#reaching-the-outside-world-for-pip-packages","text":"You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export HTTP_PROXY = http://theta-proxy.tmi.alcf.anl.gov:3128 export HTTPS_PROXY = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py","title":"Reaching the outside world for pip packages"},{"location":"building_python_packages/#building-custom-packages","text":"Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.","title":"Building custom packages"},{"location":"building_python_packages/#hdf5","text":"You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!","title":"HDF5"},{"location":"building_python_packages/#horovod","text":"Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Horovod"},{"location":"gpu_node_queue_and_policy/","text":"GPU Node Queue and Policy Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary. Nodes vs Queue vs MIG mode The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script. Queues There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 24 hours MaxRunning will be 2 full nodes or 16 individual GPUs Queue Restrictions MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"GPU Node Queue and Policy"},{"location":"gpu_node_queue_and_policy/#gpu-node-queue-and-policy","text":"Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.","title":"GPU Node Queue and Policy"},{"location":"gpu_node_queue_and_policy/#nodes-vs-queue-vs-mig-mode","text":"The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script.","title":"Nodes vs Queue vs MIG mode"},{"location":"gpu_node_queue_and_policy/#queues","text":"There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 24 hours MaxRunning will be 2 full nodes or 16 individual GPUs","title":"Queues"},{"location":"gpu_node_queue_and_policy/#queue-restrictions","text":"MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"Queue Restrictions"},{"location":"mpi/","text":"Launching a Singularity container with MPI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash SINGULARITYBIN = $( which singularity ) CONTAINER = ${ HOME } /singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent = $( cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent = ${ OMPI_MCA_orte_launch_agent } $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"MPI"},{"location":"mpi/#launching-a-singularity-container-with-mpi","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash SINGULARITYBIN = $( which singularity ) CONTAINER = ${ HOME } /singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent = $( cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent = ${ OMPI_MCA_orte_launch_agent } $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"Launching a Singularity container with MPI"},{"location":"system_overview_and_node_information/","text":"ThetaGPU Machine Overview ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect. Login Nodes The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( thetagpusn1 or thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"ThetaGPU"},{"location":"system_overview_and_node_information/#thetagpu","text":"","title":"ThetaGPU"},{"location":"system_overview_and_node_information/#machine-overview","text":"ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect.","title":"Machine Overview"},{"location":"system_overview_and_node_information/#login-nodes","text":"The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( thetagpusn1 or thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"Login Nodes"},{"location":"Building_Compiling/","text":"Description These are the steps to build code that has Python/C++ code interoperability. 1. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized tensorflow. These are: 3. Activate the TensorFlow 2.2 singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export HTTP_PROXY = http : // theta - proxy . tmi . alcf . anl . gov : 3128 export HTTPS_PROXY = https : // theta - proxy . tmi . alcf . anl . gov : 3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python - m pip install -- user virtualenv export VENV_LOCATION =/ home / rmaulik / THETAGPU_TF_ENV # Add your path here python - m virtualenv -- system - site - packages $ VENV_LOCATION source $ VENV_LOCATION / bin / activate python - m pip install cmake python - m pip install matplotlib python - m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"Building_Compiling/#description","text":"These are the steps to build code that has Python/C++ code interoperability. 1. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized tensorflow. These are: 3. Activate the TensorFlow 2.2 singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export HTTP_PROXY = http : // theta - proxy . tmi . alcf . anl . gov : 3128 export HTTPS_PROXY = https : // theta - proxy . tmi . alcf . anl . gov : 3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python - m pip install -- user virtualenv export VENV_LOCATION =/ home / rmaulik / THETAGPU_TF_ENV # Add your path here python - m virtualenv -- system - site - packages $ VENV_LOCATION source $ VENV_LOCATION / bin / activate python - m pip install cmake python - m pip install matplotlib python - m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"Building_Compiling/compiling_and_linking/","text":"Compiling and Linking ThetaGPU basically has AMD processors on the service nodes (thetagpusn1,2) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"Building_Compiling/compiling_and_linking/#compiling-and-linking","text":"ThetaGPU basically has AMD processors on the service nodes (thetagpusn1,2) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/","text":"NVidia Container Notes Getting the container To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/ Running on ThetaGPU After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different. Running Tensorflow-2 with Horovod on ThetaGPU To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running with Singularity"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#nvidia-container-notes","text":"","title":"NVidia Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#getting-the-container","text":"To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/","title":"Getting the container"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-on-thetagpu","text":"After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different.","title":"Running on ThetaGPU"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-tensorflow-2-with-horovod-on-thetagpu","text":"To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running Tensorflow-2 with Horovod on ThetaGPU"},{"location":"ml_frameworks/tensorflow/running_with_conda/","text":"Running Tensorflow with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. Tensorflow (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Running with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#running-tensorflow-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs.","title":"Running Tensorflow with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#tensorflow-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release.","title":"Tensorflow (master build)"},{"location":"ml_frameworks/tensorflow/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/","text":"Tensorboard Instructions After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/#tensorboard-instructions","text":"After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard Instructions"}]}